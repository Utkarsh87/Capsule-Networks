{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet MNIST v2.0.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPy2JkvgSu2+s1snWp+kZ0s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh87/Capsule-Networks/blob/master/CapsNet%20MNIST%20v2.0.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP02CKQi3cOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N52x_hV93xDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcMd6VhJKGjA",
        "colab_type": "code",
        "outputId": "69f62dd7-8cc0-4fc7-d37b-8ad91b5021e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFHwsQ-Npw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRMiFvFpNxKN",
        "colab_type": "code",
        "outputId": "2ff04fed-524f-407a-c1f3-312ed8a66d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5YKhA3e3xFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cFnWoEwo19P",
        "colab_type": "text"
      },
      "source": [
        "Load in MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCRAEcpU3xH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "642d4582-77e5-4c21-85f4-573e6d10fd7a"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "print(train_images.shape)\n",
        "\n",
        "val_images = train_images[50000:]\n",
        "val_labels = train_labels[50000:]\n",
        "\n",
        "train_images = train_images[:50000]\n",
        "train_labels = train_labels[:50000]"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWtZM9W-o5mb",
        "colab_type": "text"
      },
      "source": [
        "Visualise a few of the MNIST images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq9HcfHD3xKv",
        "colab_type": "code",
        "outputId": "ab12f8cb-5a70-4520-b80d-6136f8b36040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "n_samples = 5\n",
        "\n",
        "plt.figure(figsize=(n_samples * 2, 3))\n",
        "for index in range(n_samples):\n",
        "    plt.subplot(1, n_samples, index + 1)\n",
        "    sample_image = train_images[index].reshape(28, 28)\n",
        "    plt.imshow(sample_image, cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnuRYa_JBHHB",
        "colab_type": "text"
      },
      "source": [
        "# Primary Capsules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuV5LqwBqd3H",
        "colab_type": "text"
      },
      "source": [
        "**Define the conv layers of the network.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqBvyMHV3xQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense\n",
        "conv_layer1 = Conv2D(256, (9, 9), strides=(1, 1), input_shape=(None, 28, 28, 1))\n",
        "conv_layer2 = Conv2D(256, (9, 9), strides=(2, 2), input_shape=(None, 20, 20, 256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ta17a4PTgdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "38a83b2b-efa3-4071-9f82-cdd8dc2ce0f5"
      },
      "source": [
        "# reshape and rescale\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "train_images = train_images*1.0/255.0\n",
        "\n",
        "val_images = val_images.reshape((val_images.shape[0], 28, 28, 1))\n",
        "val_images = val_images*1.0/255.0\n",
        "\n",
        "# batch training and validation needed due to high number of samples\n",
        "batch_size = 64\n",
        "num_training_batches = train_images.shape[0]//batch_size\n",
        "num_val_batches = val_images.shape[0]//batch_size\n",
        "\n",
        "print(num_training_batches, num_val_batches)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "781 156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMgJyAKf1ZiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_conv(x):\n",
        "  op = conv_layer1(x)\n",
        "  return op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDhZIQDK1GW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def primary_caps_conv(x):\n",
        "  op = conv_layer2(x)\n",
        "  return op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzhNmdW45svz",
        "colab_type": "text"
      },
      "source": [
        "**Now we need to squash the vectors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e0uzyD2yi-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(s, axis=-1, epsilon=1e-7):\n",
        "  \n",
        "    squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=True)\n",
        "    safe_squared_norm = tf.sqrt(squared_norm + epsilon)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = s / safe_squared_norm\n",
        "\n",
        "    return squash_factor * unit_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vch-l-LQAVOE",
        "colab_type": "text"
      },
      "source": [
        "Now we have the outputs from the Primary capsule layer(child layer) ready, to comoute the output of the next capule layer, i.e., the DigiCaps layer(parent layer), we need to get the predicted output vectors(one for each child/parent capsule pair). After having the predicted output vectors ready we can run the routing-by-agreement algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcFZJGJJA9MS",
        "colab_type": "text"
      },
      "source": [
        "# Digit Capsules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naN-PpY1Bify",
        "colab_type": "text"
      },
      "source": [
        "**Compute the predicted output vectors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3f_RtyqCGTZ",
        "colab_type": "text"
      },
      "source": [
        "The DigiCaps layer will contain 10 capsules(one for each digit), each a 16-dimensional vector. Hence the transformation matrix Wij has a shape (16, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYBm1y8qHk2D",
        "colab_type": "text"
      },
      "source": [
        "To get the predicted vectors, need to multiply the output of the primary capsule layers with the transformation matrices. The output of this matmul will be how the output of a capsule in the child layer relates spatially with the capsule in the parent layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrZukW6f7z7A",
        "colab_type": "code",
        "outputId": "08ea490b-aa7d-4f15-e7d7-2bf4d21d8bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Primary capsule layer; number of capsules and dimensionality of each capsule\n",
        "caps1_n_caps = 1152 \n",
        "caps1_n_dims = 8\n",
        "\n",
        "# Digit capsule layer; number of capsules and dimensionality of each capsule\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n",
        "\n",
        "# define the weight/transformation matrix as random numbers initially and hopefully it learns the correct weights during the training process.\n",
        "# TODO: look for a better init for weight matrix.\n",
        "\n",
        "init_sigma = 0.1\n",
        "\n",
        "W_init = tf.random.normal(\n",
        "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
        "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
        "\n",
        "print(W_init.shape)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1152, 10, 16, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Dfn98XIFZT",
        "colab_type": "text"
      },
      "source": [
        "Since there are \"batch_size\" number of images in one batch of images, need to replicate the values in the created weight matrix \"batch_size\" number of times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07tfGTuvDyB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_tiled = tf.tile(W_init, [batch_size, 1, 1, 1, 1], name=\"W_tiled\") # replicate only along the first dimension, hence kept others 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LjYAWp4EAvw",
        "colab_type": "code",
        "outputId": "dc83a607-5f84-4fba-bb05-a426d32a445f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(W_tiled.shape)\n",
        "print(type(W_tiled))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1152, 10, 16, 8)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_MGpmvIIkb8",
        "colab_type": "text"
      },
      "source": [
        "Looking at a single image: for each child capsule-parent capsule pair(total 1152*10 such pairs) we have (16, 8) transformation matrix.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is true for all the images in the batch, hence \"batch_size\" is the first dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMfYmGNjATMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tile_caps1_output(caps1_output):\n",
        "  caps1_output_expanded = tf.expand_dims(caps1_output, -1)\n",
        "  caps1_output_expanded2 = tf.expand_dims(caps1_output_expanded, 2)\n",
        "  caps1_output_tiled = tf.tile(caps1_output_expanded2, [1, 1, caps2_n_caps, 1, 1])\n",
        "  return caps1_output_tiled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqT3nmi0LiMf",
        "colab_type": "text"
      },
      "source": [
        "Earlier the shape of the primary capsule layer output was(batch_size, 1152, 8).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Tiling the output of the primary capsule layer achieves the following:\n",
        "\n",
        "Earlier there were 1152 8-D vectors as the outputs of primary capsule layer, but since the output from 1 child capsule goes to all the capsules in the parent layer, the output of each child capsule needs to replicated as many times as there are capsules in the parent layer. This is done so that the same output is sent from a child capsule to each of the capsules in the parent layer.\n",
        "\n",
        "Hence now the outputs from the primary capsule layer are such that there is an output for every (child-capsule, parent-capsule) pair with all the outputs from one particular child capsule being the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVxs1HV-B3ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caps1_prediction(caps1_output_tiled):\n",
        "  '''This function generates the child capsule's (here the primary capsule)\n",
        "  prediction/best guess of what the parent capsule's(here the digit capsule)\n",
        "  output looks like.'''\n",
        "  ''' If the predictions of all the child capsules match, then it is said that\n",
        "  the child capsules are in agreement with the pose/orientation of the parent\n",
        "  capsule and hence all of them will route their outputs to that particular\n",
        "  paarent capsule thereafter.\n",
        "  Hence this predicted output from the primary caps layer is essential to the\n",
        "  working of the routing-by-agreement algorithm.'''\n",
        "  \n",
        "  caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled)\n",
        "  return caps2_predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHHeuZtFNLVC",
        "colab_type": "text"
      },
      "source": [
        "Progress: We now have, for each (child capsule-parent capsule) pair an 8-D(8, 1) output vector which will be multiplied with a (16, 8) weight/transformation matrix to yield 16-D(16, 1) predicted output vectors.\n",
        "\n",
        "\n",
        "---\n",
        "For each instance in the batch, and for each (child capsule-parent capsule) pair, i.e. 1152*10 such pairs(number of capsules in child layer * number of capsules in parent layer)a vector, each of 16 dimensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOS4FJXbOSbt",
        "colab_type": "text"
      },
      "source": [
        "**Routing-by-agreement algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqyDW0qJG2EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the scalar weights/coefficients that will be tuned during the routing process\n",
        "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
        "                       dtype=np.float32)\n",
        "# Following from the original paper, the init of all b_i(read b-sub-i) is to zero"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcUm5zchPoI7",
        "colab_type": "text"
      },
      "source": [
        "For every sample in the batch, and for each of the (child capsule, parent\n",
        "capsule) pairs there is a routing weight.\n",
        "Hence: for every sample in the batch; the number of routing weights is:\n",
        "\n",
        "number of capsules in the child layer * number of capsules in the parent layer(here = 1152*10)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAW0ic2kMQoc",
        "colab_type": "text"
      },
      "source": [
        "[Aurelion Geron explains the following step beautifully:](https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb)\n",
        "\n",
        "There are a couple important details to note here:\n",
        "\n",
        "To perform elementwise matrix multiplication (also called the Hadamard product, noted $\\circ$), we use the tf.multiply() function. It requires routing_weights and caps2_predicted to have the same rank, which is why we added two extra dimensions of size 1 to routing_weights, earlier.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The shape of routing_weights is (batch size, 1152, 10, 1, 1) while the shape of caps2_predicted is (batch size, 1152, 10, 16, 1). Since they don't match on the fourth dimension (1 vs 16), tf.multiply() automatically broadcasts the routing_weights 16 times along that dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqO3sNz1LvDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weighted_sum(caps2_predicted, routing_weights):\n",
        "  weighted_predictions = tf.multiply(routing_weights, caps2_predicted)\n",
        "  weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True)\n",
        "  return weighted_sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYIpurHoZ2O6",
        "colab_type": "text"
      },
      "source": [
        "# Estimate class probabilities(Length of the output vectors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuMDZxeCaDMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def safe_norm(s, axis=-1, epsilon=1e-8, keepdims=False):\n",
        "    squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=keepdims)\n",
        "    return tf.sqrt(squared_norm + epsilon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMdz35I1bs-G",
        "colab_type": "text"
      },
      "source": [
        "# Margin loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxoJp88IbxJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m_plus = 0.9\n",
        "m_minus = 0.1\n",
        "lambda_ = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL868eRvdN_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def margin_loss(caps2_output, T):\n",
        "  caps2_output_norm = safe_norm(caps2_output, axis=-2, keepdims=True)\n",
        "  present_error_raw = tf.square(tf.maximum(0., m_plus-caps2_output_norm))\n",
        "  present_error = tf.reshape(present_error_raw, shape=(-1, 10))\n",
        "  absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm-m_minus))\n",
        "  absent_error = tf.reshape(absent_error_raw, shape=(-1, 10))\n",
        "  # compute the loss for each instance and each digit\n",
        "  loss_temp = tf.add(T*present_error, lambda_*(1.0-T)*absent_error)\n",
        "  # compute mean loss over all instances\n",
        "  loss_final = tf.reduce_mean(tf.reduce_sum(loss_temp, axis=1))\n",
        "  return loss_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMOcySXW1v-R",
        "colab_type": "code",
        "outputId": "72310058-637d-49fe-f2dc-81dd4d58853c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "routing_iterations = 3 # minimum advisable\n",
        "epochs = 5\n",
        "train_loss = []\n",
        "\n",
        "num_training_batches = 10 # force it to a lesser value for debugging\n",
        "\n",
        "for i in range(num_training_batches):\n",
        "  # TRAINING PASS\n",
        "\n",
        "  # Create the input batch\n",
        "  ip = train_images[i*batch_size : (i+1)*batch_size]\n",
        "\n",
        "  # Feed to ReLU CONV1\n",
        "  intermediate_op = relu_conv(ip)\n",
        "  # Feed to Primary Capsule Layer\n",
        "  op = primary_caps_conv(intermediate_op)\n",
        "\n",
        "  # Reshape\n",
        "  op = tf.reshape(op, [batch_size, 1152, 8])  \n",
        "\n",
        "  # Squash the op of primary caps layer to get raw primary caps output\n",
        "  caps1_output = squash(op)\n",
        "\n",
        "  # Tile(replicate) the output of primary capsule layer\n",
        "  caps1_output_tiled = tile_caps1_output(caps1_output)\n",
        "\n",
        "  # Get the PrimaryCaps layer's prediction of the DigitCaps layer's output.\n",
        "  caps2_predicted = caps1_prediction(caps1_output_tiled)\n",
        "\n",
        "  # ********************************************************************\n",
        "  # ******************** START DYNAMIC ROUTING *************************\n",
        "  # ********************************************************************\n",
        "\n",
        "  for route_iter in range(routing_iterations):\n",
        "    # Softmax\n",
        "    routing_weights = tf.nn.softmax(raw_weights, axis=2, name=\"routing_weights\")\n",
        "    weighted_sum = get_weighted_sum(caps2_predicted, routing_weights)\n",
        "\n",
        "    caps2_output = squash(weighted_sum, axis=-2)\n",
        "    if route_iter < routing_iterations:\n",
        "      # For the shapes to match, need to tile the caps2_output 1152 times.\n",
        "      caps2_output_tiled = tf.tile(caps2_output,\n",
        "                                   [1, caps1_n_caps, 1, 1, 1])\n",
        "      # calculate the agreement\n",
        "      agreement = tf.linalg.matmul(caps2_predicted, caps2_output_tiled, \n",
        "                                  transpose_a=True, name=\"agreement\")\n",
        "      # Agreement is used to update the raw weights for next iteration.\n",
        "      raw_weights = tf.add(raw_weights, agreement)\n",
        "\n",
        "  # ********************************************************************\n",
        "  # ********************* END OF DYNAMIC ROUTING ***********************\n",
        "  # ********************************************************************\n",
        "\n",
        "  # Estimate class probabilities(length of the output vectors).\n",
        "  # The output vector with the greatest length is the class/label predicted\n",
        "  # by the dynamic routing algorithm.\n",
        "  y_prob = safe_norm(caps2_output, axis=-2)\n",
        "  # Get the final prediction\n",
        "  _argmax = tf.argmax(y_prob, axis=-2)\n",
        "  y_pred = tf.squeeze(_argmax, axis=[1, 2])\n",
        "  # holds the predictions for each image in the batch.\n",
        "\n",
        "  # create one-hot labels for the 10 digit classes\n",
        "  indices = train_labels[i*batch_size : (i+1)*batch_size]\n",
        "  depth = 10\n",
        "  T = tf.one_hot(indices, depth)\n",
        "\n",
        "  caps_net_loss = margin_loss(caps2_output, T)\n",
        "  # TODO: BACKPROP !!\n",
        "  train_loss.append(caps_net_loss.numpy())\n",
        "\n",
        "  # VALIDATION PASS\n",
        "\n",
        "print(np.mean(train_loss))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8087934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NohMwNEcVzV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}