{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/I2T4tM99wHpg7wdJnP00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh87/Capsule-Networks/blob/master/CapsNet%20MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP02CKQi3cOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N52x_hV93xDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5YKhA3e3xFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cFnWoEwo19P",
        "colab_type": "text"
      },
      "source": [
        "Load in MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCRAEcpU3xH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWtZM9W-o5mb",
        "colab_type": "text"
      },
      "source": [
        "Visualise a few of the MNIST images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq9HcfHD3xKv",
        "colab_type": "code",
        "outputId": "e2fc34af-2467-4154-892a-390add4d641d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "n_samples = 5\n",
        "\n",
        "plt.figure(figsize=(n_samples * 2, 3))\n",
        "for index in range(n_samples):\n",
        "    plt.subplot(1, n_samples, index + 1)\n",
        "    sample_image = train_images[index].reshape(28, 28)\n",
        "    plt.imshow(sample_image, cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAABuCAYAAAAj1slPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOw0lEQVR4nO3de2xURRvH8YEqdylisQGpGJW0okQEEQVRQQU1oCAKJBQpF9MoQhPBiuCFYDVYxUQtRFEDgoIQIhrQiJXIxYBQL4VKYqlGEIIgUEsVLwjl/et9fGbsbrdld8/u9Pv563fy7J6OrutOztyanD592gAAAPisadANAAAAiDU6PAAAwHt0eAAAgPfo8AAAAO/R4QEAAN6jwwMAALx3Vh111qwHr0kU78XnGbxofZ58lsHju+kXvpv+qPWz5AkPAADwHh0eAADgPTo8AADAe3R4AACA9+jwAAAA79HhAQAA3qPDAwAAvEeHBwAAeI8ODwAA8B4dHgAA4D06PAAAwHt0eAAAgPfqOjwUSFhfffWV5KKiIqv21ltvSR43bpxVmzJliuSePXvGqHUAgETCEx4AAOA9OjwAAMB7TU6fPh2uHraYKE6dOiX52LFjEb3HHQL5448/JJeXl1u1+fPnS54+fbpVW758ueQWLVpYtRkzZkh+6qmnImpXLZo09I21SIrPM5TS0lLresCAAZKrq6sjvk9qaqrkysrKM29Y/UTr80zqzzIW1q9fb12PGTNG8saNG61aZmZmNP4k380zVFBQYF0/+eSTkt3fpg0bNki+8cYbY9Ecvpv+qPWz5AkPAADwHh0eAADgPTo8AADAewm1LP2nn36SfOLECau2ZcsWyZ9//rlVq6qqkrxq1aozbkdGRoZ1rZcxr1692qqdc845kq+88kqrFqNx5kZl+/btkkeMGGHV9HytJk3sIdu2bdtKbtasmVU7cuSI5K1bt1q1Xr16hXyfDzZt2iT56NGjVm348OHxbk5UlZSUWNdXX311QC1BOIsXL5Y8d+5cq5aSkiJZz8005r/fcaC+eMIDAAC8R4cHAAB4L9AhrW+++ca6HjhwoORIl5dHi36U6i6VbN26tWS91NUYYzp16iT53HPPtWpRWvrqPb0lgDHGfP3115Kzs7MlHzhwIOJ7du3aVXJ+fr5VGzVqlOR+/fpZNf3Zz5w5M+K/lyz00t6KigqrloxDWjU1NZJ//PFHq6aHyOvYfgNxtHfvXsl///13gC1p3LZt2yZ56dKlkvWwtzHGfPvttyHvMW/ePMn6t9AYYzZv3ix57NixVq1Pnz71a2yU8IQHAAB4jw4PAADwHh0eAADgvUDn8HTp0sW6TktLkxyNOTzuOKGeY/PZZ59ZNb0E2R1vRGzl5uZa18uWLTvje+qT1H///XerprcL0HNajDGmrKzsjP92ItOnyPft2zfAlkTHzz//LHnhwoVWTX+Ps7Ky4tYm2D799FPr+uWXXw75Wv05rV271qqlp6dHt2GNzIoVK6zrvLw8yYcPH5bszne76aabJOstPYz571FLmr6P+75333237gbHAE94AACA9+jwAAAA7wU6pNW+fXvr+vnnn5e8Zs0aq3bVVVdJnjp1ash79ujRQ7L7KFUvL3eX2oV7zIro00NO7qPrUEuI9aNVY4wZMmSIZPfRql4iqf/bMSb80Kbvy5f1Mm4fTJo0KWRNb02A+NK74efk5Fi16urqkO975JFHJLtTHlC3kydPWtd69/H777/fqh0/flyyHuZ/4oknrNddf/31kt1tBEaOHCl53bp1IduVKLue84QHAAB4jw4PAADwHh0eAADgvYQ6LX3YsGGS9TETxtinku/cudOqvfHGG5L1XA49Z8d1xRVXWNfuklZEV2lpqXV9yy23SHbH9PWpyHfccYfk5cuXW6/TS8qfeeYZq6bndnTo0MGq6VPt3ROYP/zwQ8n6iAtjjOnZs6dJNu535dChQwG1JDaqqqpC1m699dY4tgSa3v4g3JEw7ry8++67L1ZNahTefvtt63rixIkhXzto0CDJesl627ZtQ77HXdoebt5ORkaG5HHjxoV8XTzxhAcAAHiPDg8AAPBeQg1paeEeq6Wmpoas6eGt0aNHW7WmTenfxdPu3bslFxYWWjW9k7Y75NSxY0fJ+lFomzZtrNfpZek6nwl9cvsLL7xg1aKxA3S8ffTRR9b1n3/+GVBLosMdktuzZ0/I115wwQUxbg3+z91J980335SckpJi1dq1ayf58ccfj23DGgH97/DZZ5+1anrIfvLkyVatoKBAcrjfW82dOhCO3urF/X98UOgBAAAA79HhAQAA3qPDAwAAvJewc3jCmT17tnWtjynQS5XdoyX0MjxEn7vtuN4iQC/3NsYeM16yZIlV09uQBznnZN++fYH97WgpLy8PWbv88svj2JLocI8QOXjwoOTMzEyrpreyQPTp+VN33313xO+bMmWKZHf7EdRtzpw51rWet9O8eXOrNnjwYMnPPfecVWvZsmWt9//rr7+s608++UTy3r17rZo+isc9kuKuu+6q9f5B4gkPAADwHh0eAADgvaQc0nJ3UH799dcl691w3dNhBwwYINk9vVUv2XN330Vk3J2J3WEs7YMPPpCsT+pF/PTu3TvoJgi92/bHH39s1fTusfrxustd4qyXPyP69OdUVlYW8nU333yzdZ2XlxezNvlK7yi+YMECq6Z/r/QQljHGvP/++xHd//vvv5c8ZswYq/bll1+GfN+9994rOT8/P6K/FSSe8AAAAO/R4QEAAN5LyiEt1yWXXCJ58eLFksePH2+9Tq8GclcGHT9+XLJ7gJ3e+RehPfzww9a1nsHvHhKYKMNYuo31qfmgsrKyQe/bsWOHdV1TUyN5/fr1Vm3//v2ST5w4Ifmdd94JeQ939UifPn0ku6tQ/vnnH8nuMDWiTw+RzJgxI+Tr+vfvL1kfJGpM+J3yUTv93Tl8+HDI1+ndjY0x5pdffpG8aNEiq6anFezatUvyb7/9Zr1OD5m5pxVkZ2dLDndYd6LgCQ8AAPAeHR4AAOA9OjwAAMB7Xszh0YYPHy750ksvtWrTpk2T7O7C/Nhjj0l2d5OcNWuWZE5gtq1du1ZyaWmpVdNjv3feeWfc2lQf7hYE+rpHjx7xbk7UufNh9D9fbm6uVXNPWg7FncOj5zqdffbZVq1Vq1aSL7vsMskTJkywXterVy/J7nyv9PR0yZ07d7ZqeifurKysupqOenJPo490R+WLL75Ysv780DDNmjWTfP7551s1PU/noosusmqRbrGif9fck9MPHDggOS0tzaoNHTo0ovsnCp7wAAAA79HhAQAA3vNuSEvr3r27db1y5UrJa9assWo5OTmSX331VatWUVEhubi4OIotTH56SEEvnTTGfvQ6atSouLXJ5R5q6h4+q+ldYefOnRurJsWNuytrly5dJG/ZsqVB97zwwguta31IYLdu3azatdde26C/oS1cuFCyfnxvjD10guhzD5xMSUmJ6H3hlqyj/vSu4e7uyUOGDJF89OhRq6andbiHeerfvPbt20sePXq09To9pOXWkg1PeAAAgPfo8AAAAO/R4QEAAN7zeg6PS4+Djh071qpNmjRJst6u3hhjNm3aJHnDhg1WzV1Ci3+1aNFCcryP59DzdgoKCqxaYWGh5IyMDKumty5o06ZNjFoXnEcffTToJtSbe1yFds8998SxJY2D3l5i3bp1Eb3H3XYiMzMzqm3Cv/RRK8aEP2oiUvo3buPGjVZNL21P9jlzPOEBAADeo8MDAAC85/WQ1s6dO63rVatWSS4pKbFq7jCWppfa3nDDDVFqnf/iubuyu8uzHrZasWKFVdPLM997773YNgwxNWzYsKCb4J1BgwZJ/vXXX0O+Tg+tuCeiI7no7UXC7T7PsnQAAIAER4cHAAB4jw4PAADwnhdzeMrLyyW/8sorkt35GQcPHozofmedZf9r0Uuqmzalj6jpk7J1NsbeAv2ll16K+t9+8cUXJT/99NNW7dixY5Kzs7Ot2pIlS6LeFsAXR44ckRzuKInJkydL9nELh8Zk8ODBQTchLvj1BgAA3qPDAwAAvJc0Q1p6OGrZsmVWraioSPKePXsadP/evXtLnjVrllWL5/LqZKOXLLrLGfVnNnXqVKs2YcIEyeedd55V++KLLyQvXbpU8o4dO6zX7du3T7I+BdwYY2677TbJDz74YOh/ACS1iooKydddd12ALUle48ePt6710PSpU6dCvq9v374xaxPiK9IdtZMdT3gAAID36PAAAADv0eEBAADeS6g5PIcOHZK8a9cuq/bQQw9J/u677xp0f70Ven5+vlXTxw2w9Dw6Tp48KXn+/PlWTR/zkZqaatV2794d0f31HIKBAwdatTlz5kTcTiSvmpqaoJuQlPRRLMXFxVZNz8Vr3ry5VdPz4dLT02PUOsTbDz/8EHQT4oJfdgAA4D06PAAAwHtxH9KqrKyUnJuba9X0Y9aGPmLr16+f5GnTplk1vZtky5YtG3R/2PRS4Guuucaqbd++PeT79JJ1PZTpSktLk+ye1BuL3ZuRXLZu3So5JycnuIYkmaqqKsnhvn+dOnWyrufNmxezNiE4/fv3l+zumO8TnvAAAADv0eEBAADeo8MDAAC8F5M5PNu2bZNcWFho1UpKSiTv37+/Qfdv1aqVda2PLdDHQrRu3bpB90fkOnfuLNk9nf61116T7J5mHk5eXp7kBx54QHLXrl0b0kQAQBjdu3eX7P5/Vs+ndefWdujQIbYNizKe8AAAAO/R4QEAAN6LyZDW6tWra8116datm+ShQ4datZSUFMnTp0+3au3atatvExEDHTt2tK5nz55dawbq4/bbb5e8cuXKAFvij6ysLMnuqeebN2+Od3OQQGbOnGldT5w4MWStqKhIsv79TlQ84QEAAN6jwwMAALxHhwcAAHivSR3bSPu7x3TyaFL3SyLG5xm8aH2efJbB47vpF76bxpjq6mrreuTIkZKLi4ut2ogRIyQvWrTIqgW8LUytnyVPeAAAgPfo8AAAAO8xpJX4eGzuFx6b+4Pvpl/4btZCD3HpkwyMMWbBggWSy8rKrFrAy9QZ0gIAAI0THR4AAOA9OjwAAMB7zOFJfMwT8AvzBPzBd9MvfDf9wRweAADQONHhAQAA3qtrSAsAACDp8YQHAAB4jw4PAADwHh0eAADgPTo8AADAe3R4AACA9+jwAAAA7/0P3GrmIvMqKdgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnuRYa_JBHHB",
        "colab_type": "text"
      },
      "source": [
        "# Primary Capsules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuV5LqwBqd3H",
        "colab_type": "text"
      },
      "source": [
        "**Define the conv layers of the network.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqBvyMHV3xQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_layer1 = tf.keras.layers.Conv2D(256, (9, 9), strides=(1, 1), input_shape=(None, 28, 28, 1))\n",
        "conv_layer2 = tf.keras.layers.Conv2D(256, (9, 9), strides=(2, 2), input_shape=(None, 20, 20, 256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ta17a4PTgdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape and rescale\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "train_images = train_images*1.0/255.0\n",
        "# print(train_images.shape)\n",
        "\n",
        "# batch training needed due to high number of training samples\n",
        "batch_size = 64\n",
        "samples_per_batch = train_images.shape[0]//batch_size\n",
        "\n",
        "num_batches = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMgJyAKf1ZiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_conv(x):\n",
        "  op = conv_layer1(x)\n",
        "  return op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDhZIQDK1GW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def primary_caps_conv(x):\n",
        "  op = conv_layer2(x)\n",
        "  return op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzhNmdW45svz",
        "colab_type": "text"
      },
      "source": [
        "**Now we need to squash the vectors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e0uzyD2yi-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# Squash function implementation taken from XifengGuo\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vch-l-LQAVOE",
        "colab_type": "text"
      },
      "source": [
        "Now we have the outputs from the Primary capsule layer(child layer) ready, to comoute the output of the next capule layer, i.e., the DigiCaps layer(parent layer), we need to get the predicted output vectors(one for each child/parent capsule pair). After having the predicted output vectors ready we can run the routing-by-agreement algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcFZJGJJA9MS",
        "colab_type": "text"
      },
      "source": [
        "# Digit Capsules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naN-PpY1Bify",
        "colab_type": "text"
      },
      "source": [
        "**Compute the predicted output vectors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3f_RtyqCGTZ",
        "colab_type": "text"
      },
      "source": [
        "The DigiCaps layer will contain 10 capsules(one for each digit), each a 16-dimensional vector. Hence the transformation matrix Wij has a shape (16, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYBm1y8qHk2D",
        "colab_type": "text"
      },
      "source": [
        "To get the predicted vectors, need to multiply the output of the primary capsule layers with the transformation matrices. The output of this matmul will be how the output of a capsule in the child layer relates spatially with the capsule in the parent layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrZukW6f7z7A",
        "colab_type": "code",
        "outputId": "44c4f8f6-52a8-4a09-c235-6f28311c6516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# define the weight/transformation matrix as random numbers initially and hopefully it learns the correct weights during the training process.\n",
        "# TODO: look for a better init for weight matrix.\n",
        "\n",
        "# Primary capsule layer; number of capsules and dimensionality of each capsule\n",
        "caps1_n_caps = 1152 \n",
        "caps1_n_dims = 8\n",
        "\n",
        "# Digit capsule layer; number of capsules and dimensionality of each capsule\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n",
        "\n",
        "init_sigma = 0.1\n",
        "\n",
        "W_init = tf.random.normal(\n",
        "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
        "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
        "W = tf.Variable(W_init, name=\"W\")\n",
        "\n",
        "print(W_init.shape)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1152, 10, 16, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Dfn98XIFZT",
        "colab_type": "text"
      },
      "source": [
        "Since there are \"batch_size\" number of images in one batch of images, need to replicate the values in the created weight matrix \"batch_size\" number of times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07tfGTuvDyB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\") # replicate only along the first dimension, hence kept others 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LjYAWp4EAvw",
        "colab_type": "code",
        "outputId": "454c3bc3-5d4e-4b18-9b6f-76cfd3fb0d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(W_tiled.shape)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1152, 10, 16, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_MGpmvIIkb8",
        "colab_type": "text"
      },
      "source": [
        "Looking at a single image: for each child capsule-parent capsule pair(total 1152*10 such pairs) we have (16, 8) transformation matrix.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is true for all the images in the batch, hence \"batch_size\" is the first dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMfYmGNjATMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tile_caps1_output(caps1_output):\n",
        "  caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
        "                                      name=\"caps1_output_expanded\")\n",
        "  # print(caps1_output_expanded.shape)\n",
        "\n",
        "\n",
        "  caps1_output_expanded2 = tf.expand_dims(caps1_output_expanded, 2,\n",
        "                                  name=\"caps1_output_tile\")\n",
        "  # print(caps1_output_expanded2.shape)\n",
        "\n",
        "\n",
        "  caps1_output_tiled = tf.tile(caps1_output_expanded2, [1, 1, caps2_n_caps, 1, 1],\n",
        "                            name=\"caps1_output_tiled\")\n",
        "  return caps1_output_tiled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqT3nmi0LiMf",
        "colab_type": "text"
      },
      "source": [
        "Earlier the shape of the primary capsule layer output was(batch_size, 1152, 8).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Tiling the output of the primary capsule layer achieves the following:\n",
        "\n",
        "Earlier there were 1152 8-D vectors as the outputs of primary capsule layer, but since the output from 1 child capsule goes to all the capsules in the parent layer, the output of each child capsule needs to replicated as many times as there are capsules in the parent layer. This is done so that the same output is sent from a child capsule to each of the capsules in the parent layer.\n",
        "\n",
        "Hence now the outputs from the primary capsule layer are such that there is an output for every (child-capsule, parent-capsule) pair with all the outputs from one particular child capsule being the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVxs1HV-B3ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caps1_prediction(caps1_output_tiled):\n",
        "  ''' generate the prediction of the parent capsule based on the output from\n",
        "  the child capsule.\n",
        "  Hence this function essentially generates the child capsule's\n",
        "  (here the primary capsule) prediction/best guess of what the parent\n",
        "  capsule's(here the digit capsule) output looks like.'''\n",
        "  ''' If the predictions of all the child capsules match, then it is said that\n",
        "  the child capsules are in agreement with the pose/orientation of the parent\n",
        "  capsule and hence all of them will route their outputs to that particular\n",
        "  paarent capsule thereafter.\n",
        "  Hence this predicted output from the primary caps layer is essential to the\n",
        "  working of the routing-by-agreement algorithm.'''\n",
        "  caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled)\n",
        "  return caps2_predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMOcySXW1v-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1e27e985-0cab-46c2-cd6c-bf3eca9bc0a6"
      },
      "source": [
        "for i in range(num_batches):\n",
        "  # Create the input batch\n",
        "  ip = train_images[i*batch_size : (i+1)*batch_size]\n",
        "\n",
        "  # Feed to ReLU CONV1\n",
        "  intermediate_op = relu_conv(ip)\n",
        "  # Feed to Primary Capsule Layer\n",
        "  op = primary_caps_conv(intermediate_op)\n",
        "\n",
        "  # Reshape\n",
        "  op = tf.reshape(op, [batch_size, 1152, 8])  \n",
        "\n",
        "  # Squash the op of primary caps layer to get raw primary caps output\n",
        "  caps1_output = squash(op)\n",
        "  # print(\"Primary Caps Layer output shape: \", caps1_output.get_shape().as_list())\n",
        "\n",
        "  # Tile(replicate) the output of primary capsule layer\n",
        "  caps1_output_tiled = tile_caps1_output(caps1_output)\n",
        "\n",
        "  # Get the PrimaryCaps layer's prediction of the DigitCaps layer's output.\n",
        "  caps2_predicted = caps1_prediction(caps1_output_tiled)\n",
        "  print(\"Sanity check; Caps2(Digit Caps) output tiled shape: \", caps2_predicted.shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sanity check; Caps2(Digit Caps) output tiled shape:  (64, 1152, 10, 16, 1)\n",
            "Sanity check; Caps2(Digit Caps) output tiled shape:  (64, 1152, 10, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHHeuZtFNLVC",
        "colab_type": "text"
      },
      "source": [
        "Progress: We now have, for each (child capsule-parent capsule) pair an 8-D(8, 1) output vector which will be multiplied with a (16, 8) weight/transformation matrix to yield 16-D(16, 1) predicted output vectors.\n",
        "\n",
        "\n",
        "---\n",
        "For each instance in the batch, and for each (child capsule-parent capsule) pair, i.e. 1152*10 such pairs(number of capsules in child layer * number of capsules in parent layer)a vector, each of 16 dimensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOS4FJXbOSbt",
        "colab_type": "text"
      },
      "source": [
        "**Routing-by-agreement algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqyDW0qJG2EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}