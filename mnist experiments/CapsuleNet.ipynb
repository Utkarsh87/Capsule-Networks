{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsuleNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOu8g5D2aH3Op767rDWIqr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh87/Capsule-Networks/blob/master/CapsuleNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEX2kaJLcsro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras==2.2.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0bnKR0EeZdc",
        "colab_type": "code",
        "outputId": "bdf6d307-6531-4946-f4a9-85e024361672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4OUHBc9d2bQ",
        "colab_type": "code",
        "outputId": "a14d3fbe-034a-4d7f-993e-406dfb23aeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import keras\n",
        "print(keras.__version__)\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlVkq0X9eDat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from keras import callbacks\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWo3ksAKeyEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import initializers, layers\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
        "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
        "    output: shape=[dim_1, ..., dim_{n-1}]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
        "    Output shape: [None, d2]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
        "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of vectors of capsules\n",
        "            x = inputs\n",
        "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
        "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
        "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
        "\n",
        "        # masked inputs, shape = [batch_size, dim_vector]\n",
        "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
        "        return inputs_masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][-1]])\n",
        "        else:\n",
        "            return tuple([None, input_shape[-1]])\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
        "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
        "    :param num_routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_vector = dim_vector\n",
        "        self.num_routing = num_routing\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_vector = input_shape[2]\n",
        "\n",
        "        # Transformation matrix/Weight matrix\n",
        "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
        "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    name='bias',\n",
        "                                    trainable=False)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
        "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
        "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
        "\n",
        "        \"\"\"  \n",
        "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
        "        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
        "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
        "        \n",
        "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
        "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
        "        \"\"\"\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
        "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
        "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
        "                             elems=inputs_tiled,\n",
        "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
        "        \"\"\"\n",
        "        # Routing algorithm V1. Use tf.while_loop in a dynamic way.\n",
        "        def body(i, b, outputs):\n",
        "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
        "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
        "            b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
        "            return [i-1, b, outputs]\n",
        "\n",
        "        cond = lambda i, b, inputs_hat: i > 0\n",
        "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
        "        _, _, outputs = tf.while_loop(cond, body, loop_vars)\n",
        "        \"\"\"\n",
        "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
        "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
        "        for i in range(self.num_routing):\n",
        "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
        "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
        "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
        "\n",
        "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
        "            if i != self.num_routing - 1:\n",
        "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
        "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
        "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
        "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_vector])\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_vector: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
        "    return layers.Lambda(squash)(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WIfQz-he-QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers, models\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "def CapsNet(input_shape, n_class, num_routing):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 4d, [None, width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param num_routing: number of routing iterations\n",
        "    :return: A Keras Model with 2 inputs and 2 outputs\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
        "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='out_caps')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
        "    x_recon = layers.Dense(512, activation='relu')(masked)\n",
        "    x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
        "    x_recon = layers.Dense(784, activation='sigmoid')(x_recon)\n",
        "    x_recon = layers.Reshape(target_shape=[28, 28, 1], name='out_recon')(x_recon)\n",
        "\n",
        "    # two-input-two-output keras Model\n",
        "    return models.Model([x, y], [out_caps, x_recon])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLYuY3MgfFxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlYJ0LiTfJni",
        "colab_type": "code",
        "outputId": "e319879e-edc1-4b5e-bd76-1bd9f8e220fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "# define model\n",
        "model = CapsNet(input_shape=[28, 28, 1],\n",
        "                n_class=10,\n",
        "                num_routing=3)\n",
        "model.summary()\n",
        "try:\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "except Exception as e:\n",
        "    print('No fancy plot {}'.format(e))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-4-8daadbc5443a>:132: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1152, 8)      0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1152, 8)      0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (None, 10, 16)       1486080     lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask_1 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          8704        mask_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         525312      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 784)          803600      dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 10)           0           digitcaps[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "out_recon (Reshape)             (None, 28, 28, 1)    0           dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 8,153,360\n",
            "Trainable params: 8,141,840\n",
            "Non-trainable params: 11,520\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HOKZDPzfMWQ",
        "colab_type": "code",
        "outputId": "a64f7c46-20c1-4b23-b364-be7d8e884189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "print(train_images.shape)\n",
        "\n",
        "val_images = train_images[50000:]\n",
        "val_labels = train_labels[50000:]\n",
        "\n",
        "train_images = train_images[:50000]\n",
        "train_labels = train_labels[:50000]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_ToTnYftDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.reshape((-1, 28, 28, 1)).astype('float32') / 255.\n",
        "test_images = test_images.reshape((-1, 28, 28, 1)).astype('float32') / 255.\n",
        "val_images = val_images.reshape((-1, 28, 28, 1)).astype('float32') / 255.\n",
        "\n",
        "train_labels = to_categorical(train_labels.astype('float32'))\n",
        "test_labels = to_categorical(test_labels.astype('float32'))\n",
        "val_labels = to_categorical(val_labels.astype('float32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM6rKK8fgJlE",
        "colab_type": "code",
        "outputId": "c321cde2-0fa0-44bc-b750-ac04698f21a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "print(train_images.shape)\n",
        "print(type(train_images))\n",
        "print(test_images.shape)\n",
        "print(type(test_images))\n",
        "print(val_images.shape)\n",
        "print(type(val_images))\n",
        "\n",
        "x_train = train_images\n",
        "y_train = train_labels\n",
        "\n",
        "x_val = val_images\n",
        "y_val = val_labels\n",
        "\n",
        "x_test = test_images\n",
        "y_test = test_labels"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 28, 28, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "(10000, 28, 28, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "(10000, 28, 28, 1)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ut3ma9Uj6i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9W7qa3phJ5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data, epoch_size_frac=1.0):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    (x_train, y_train), (x_val, y_val) = data\n",
        "\n",
        "    # callbacks\n",
        "    # log = callbacks.CSVLogger('log.csv')\n",
        "    checkpoint = callbacks.ModelCheckpoint('weights-{epoch:02d}.h5', save_best_only=True, save_weights_only=True, verbose=1)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., 0.0005],\n",
        "                  metrics={'out_caps': 'accuracy'})\n",
        "\n",
        "    # Training without data augmentation:\n",
        "    history = model.fit([x_train, y_train], [y_train, x_train], batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=[[x_val, y_val], [y_val, x_val]], callbacks=[lr_decay, checkpoint])\n",
        "\n",
        "    model.save_weights('trained_model.h5')\n",
        "    print('Trained model saved to \\'trained_model.h5\\'')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0otqEo0qidyg",
        "colab_type": "code",
        "outputId": "ff836d0d-b061-47c0-cde0-3c25c4a2ca34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "train(model=model, data=((x_train, y_train), (x_val, y_val)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "50000/50000 [==============================] - 207s 4ms/step - loss: 0.0906 - out_caps_loss: 0.0906 - out_recon_loss: 0.1471 - out_caps_acc: 0.9116 - val_loss: 0.0222 - val_out_caps_loss: 0.0222 - val_out_recon_loss: 0.0663 - val_out_caps_acc: 0.9856\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02220, saving model to weights-01.h5\n",
            "Epoch 2/3\n",
            "50000/50000 [==============================] - 192s 4ms/step - loss: 0.0192 - out_caps_loss: 0.0191 - out_recon_loss: 0.0661 - out_caps_acc: 0.9869 - val_loss: 0.0153 - val_out_caps_loss: 0.0153 - val_out_recon_loss: 0.0641 - val_out_caps_acc: 0.9890\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02220 to 0.01528, saving model to weights-02.h5\n",
            "Epoch 3/3\n",
            "50000/50000 [==============================] - 192s 4ms/step - loss: 0.0130 - out_caps_loss: 0.0130 - out_recon_loss: 0.0635 - out_caps_acc: 0.9913 - val_loss: 0.0141 - val_out_caps_loss: 0.0141 - val_out_recon_loss: 0.0609 - val_out_caps_acc: 0.9900\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01528 to 0.01414, saving model to weights-03.h5\n",
            "Trained model saved to 'trained_model.h5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f86dbe7f0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmxIwsheCnYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "04014411-34d2-4d0e-ee51-cb6320b52dd3"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.png    trained_model.h5  weights-02.h5\n",
            "sample_data  weights-01.h5     weights-03.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C04TLbRGPW_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "b9e5ba0d-19df-42ae-a794-046f39f249b3"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMZPLpHLGgNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('trained_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vraT-z64G1gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}